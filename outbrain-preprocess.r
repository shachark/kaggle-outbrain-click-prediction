# This prepares data for modeling, and later used by the outbrain.r file.

# NOTE: Some of the inputs to this script are summaries of the huge page_views file, generated by
# external scripts (e.g., analyze-page-views.r, extract-leak.py, and AWS queries).

needs(data.table, caret, Matrix, xgboost)

debug.mode = F # NOTE: since this code is highly memory inefficient, the full run needs ~100GB of RAM

do.merge     = T
do.engineer1 = T
do.engineer2 = T

add.negative.leak = F
add.expanded.leak = T
keep.factors = F
merge.rare.categories = F
rare.threshold = 6 # FIXME tune the threshold (I don't like thresholds...)
debug.ndisps = 1e6

#tmpdir = 'C:/TEMP/kaggle/outbrain-click-prediction'
tmpdir = 'tmp'

if (debug.mode) {
  cat('NOTE: working in debug mode, with a subset of samples\n')
}

if (do.merge) {
  cat(date(), 'Merging\n')
  
  cat(date(), 'Clicks\n')
  dat = fread('input/clicks_test.csv')
  ntest = nrow(dat)
  dat[, clicked := NA]
  dat = rbind(dat, fread('input/clicks_train.csv'))
  
  # This doesn't help.. The order (as in the original file) of ad within the display.
  # dat[, ad.display.idx := 1:.N, by = display_id]
  
  cat(date(), 'Ads\n')
  ads = fread('input/promoted_content.csv')
  if (add.negative.leak) {
    tracked.docs = fread('tracked_docids.csv')
    tracked.docs[, tracked := 1]
    ads = merge(ads, tracked.docs, by = 'document_id', all.x = T)
    ads[is.na(tracked), tracked := 0]
    rm(tracked.docs); #gc()
  }
  setnames(ads, 'document_id', 'ad_target')
  dat = merge(dat, ads, by = 'ad_id', all.x = T)
  rm(ads); #gc()
  
  if (0) {
    # FYI:
    # The Python script from the forum has one line per document_id that appears in page_views and is
    # also the target of an ad_id from promoted_content (=train+test ad_ids). Each line has the format:
    # docid,list_of_space_separated_uuids
    #
    # At least for the subset of train+test ads for which the target document also has ads on it, this
    # tells us which users visited that page during the time period of interest. Specifically, we learn
    # two things about clicks:
    # 1. Having a visit on such a page strongly suggests a click on a targeting ad (as a matter of fact
    #    we could verify that by recording the timestamp!! but it's pretty solid even without this
    #    verification). 
    # 2. Conversely, if we know an ad targets a tracked page, but we don't see a visit to that page form
    #    a certain user, this precludes the targeting ad as a click candidate for that user. (Moreover 
    #    even if we do see a visit but the time doesn't match the event then it still precludes the ad.)
    #    I thought pages are tracked if they appear in documents_meta, but this can't be true because
    #    all ad target pages are there. So the only info we have is a sufficient condition (but not 
    #    necessary) that a page is tracked if it appears in page_views. This excludes all pages that are
    #    never visited (even if they are in fact tracked). Maybe if we had "events" also for all 
    #    no-click dispalys we could know more, but we don't. I just hope that page_views is the complete
    #    log of visits, otherwise there will be many visits we don't know about, so not seeing a visit
    #    is only weakly indicative of a non-click (how weak, we won't know).
    #
    # UPDATE: I tried this, and it looks as though OB removed the direct page_views resulting from 
    # clicks, otherwise I can't explain why the negative leak has no effect.
    
    # I've awked the output of extract-leak.py to produce a long list of docid,uuid:
    # awk 'BEGIN{FS=","}{split($2, a, " "); for (i in a) print $1, a[i]}' leak.csv > processed-leak.csv
    # Now I need to merge it with the click files to add an indicator of "leak" if the (ad target docid,
    # uuid) pair is in page_views.
    #
    # In addition, to get the list of "certainly tracked" pages, I ran
    # awk -F"," '!_[$2]++{print $2}' input/page_views.csv > tracked_docids.csv
    # As it turnes out, it is not so "certain". Tracked documents that appear in page_view 
    # but not with a specific user, can still be clicked by that user about 12% of the time. This isn't
    # much of a list over the marginal click rate of about 20%. I expected such clicks to almost never
    # happen. This is very odd. It could be that we don't see the complete page_views for all the pages
    # and users. There could be a problem with uuid/document_id assignment during data prep. It could be
    # that tracking on many of these ad target pages is only active for part of the time period, so we
    # might think a page is being tracked when it is not because we mark tracking ignoring timestamps.
  }
  
  cat(date(), 'Displays\n')
  disps = fread('input/events.csv', na.strings = c('', 'NA', '\\N'))
  setnames(disps, 'document_id', 'disp_docid')
  dat = merge(dat, disps, by = 'display_id', all.x = T)
  rm(disps); #gc()
  
  cat(date(), 'Users (summary form page visits)\n')
  load(paste0(tmpdir, '/users.RData')) # => users
  setnames(users, c('uuid', paste0('user.', names(users)[-1])))
  dat = merge(dat, users, by = 'uuid', all.x = T)
  rm(users); #gc()
  
  cat(date(), 'Leaks (or whatever these are)\n')
  leak = fread(paste0(tmpdir, '/processed-leak.csv'), sep = ' ')
  setnames(leak, 'document_id', 'ad_target')
  leak[, leak := 1]
  #gc()
  dat = merge(dat, leak, by = c('ad_target', 'uuid'), all.x = T)
  #gc()
  dat[is.na(leak), leak := 0]
  leak = leak[, .(ad_target_popularity = 1 + log(.N)), by = ad_target]
  #gc()
  dat = merge(dat, leak, by = 'ad_target', all.x = T) # NOTE: due to how we generated the file, this only counts the number of unique uuids
  dat[is.na(ad_target_popularity), ad_target_popularity := 0]
  rm(leak); #gc()
  
  if (add.expanded.leak) {
    cat(date(), 'Expanded leaks: campaigns\n')
    leak = unique(fread(paste0(tmpdir, '/users-page-visits-by-campaign.csv')))
    leak[, uuid_campaign := 1]
    dat = merge(dat, leak, by = c('uuid', 'campaign_id'), all.x = T)
    dat[is.na(uuid_campaign), uuid_campaign := 0]
    rm(leak); gc()
  }
  
  cat(date(), 'Docs\n')
  docs = fread('input/documents_meta.csv')
  
  categories = fread('input/documents_categories.csv')
  #categories = categories[confidence_level > 0.2] # 0.6 # FIXME tune this or use more than one value
  setkey(categories, document_id, confidence_level)
  categories = unique(categories, by = 'document_id', fromLast = T) # TODO: docs have a max of two cats. I can therefor also try to define a combined category either as the (cat1, cat2 > cat1) or as the (cat1 high conf, cat2 low conf)
  categories = categories[, .(document_id, category_id)]
  docs = merge(docs, categories, by = 'document_id', all.x = T)
  rm(categories); #gc()
  
  # TODO: for entities and topics, instead of using only the most prominent one, use match counts and 
  # first match positions as features (for singles, and for pairs)
  # TODO: BTW, I should also add the number of topics, entities etc as features
  entities = fread('input/documents_entities.csv')
  entities = entities[confidence_level > 0.2] # 0.6 # FIXME tune this or use more than one value
  setkey(entities, document_id, confidence_level)
  entities = unique(entities, by = 'document_id', fromLast = T) # TODO: There are up to 10 ents per doc. I can bring in the set of ents, and count the matches between display page and ad target page
  entities = entities[, .(document_id, entity_id)]
  docs = merge(docs, entities, by = 'document_id', all.x = T)
  rm(entities); #gc()
  
  # NOTE: topics are more problematic: up to 39 tops per doc (but can cut at 10 or 15 easily), very low confidence
  # throughout. On the other hand, there are only 300 possible topics. Since they are typically all low confidence,
  # picking the strongest one is questionable. I can pick the most common maybe. A better approach seems to be to
  # merge source (display) and destination (ad target) 300-topic indicator vectors, and count matches or percent 
  # match or something like that.
  topics = fread('input/documents_topics.csv')
  setkey(topics, document_id, confidence_level)
  topics = unique(topics, by = 'document_id', fromLast = T)
  topics = topics[, .(document_id, topic_id)]
  docs = merge(docs, topics, by = 'document_id', all.x = T)
  rm(topics); #gc()
  
  setnames(docs, c('disp_docid', 'disp_doc_subdomain', 'disp_doc_domain', 'disp_doc_pubtime', 'disp_doc_cat', 'disp_doc_ent', 'disp_doc_top'))
  dat = merge(dat, docs, by = 'disp_docid', all.x = T)
  gc()
  setnames(docs, c('ad_target', 'ad_target_subdomain', 'ad_target_domain', 'ad_target_pubtime', 'ad_target_cat', 'ad_target_ent', 'ad_target_top'))
  dat = merge(dat, docs, by = 'ad_target', all.x = T)
  rm(docs); #gc()
  
  if (add.expanded.leak) {
    cat(date(), 'Expanded leaks: domains\n')
    leak = unique(fread(paste0(tmpdir, '/user_publishers.csv')))
    setnames(leak, c('uuid', 'ad_target_domain'))
    leak[, uuid_domain := 1]
    dat = merge(dat, leak, by = c('uuid', 'ad_target_domain'), all.x = T)
    dat[is.na(uuid_domain), uuid_domain := 0]
    rm(leak); #gc()
    
    cat(date(), 'Expanded leaks: subdomains\n')
    leak = unique(fread(paste0(tmpdir, '/user_sources.csv')))
    setnames(leak, c('uuid', 'ad_target_subdomain'))
    leak[, uuid_subdomain := 1]
    dat = merge(dat, leak, by = c('uuid', 'ad_target_subdomain'), all.x = T)
    dat[is.na(uuid_subdomain), uuid_subdomain := 0]
    rm(leak); #gc()
    
    cat(date(), 'Expanded leaks: categories\n')
    leak = unique(fread(paste0(tmpdir, '/user_categories.csv')))
    setnames(leak, c('uuid', 'ad_target_cat'))
    leak[, uuid_cat := 1]
    dat = merge(dat, leak, by = c('uuid', 'ad_target_cat'), all.x = T)
    dat[is.na(uuid_cat), uuid_cat := 0]
    rm(leak); #gc()
    
    cat(date(), 'Expanded leaks: entities\n')
    leak = unique(fread(paste0(tmpdir, '/user_entities.csv')))
    setnames(leak, c('uuid', 'ad_target_ent'))
    leak[, uuid_ent := 1]
    dat = merge(dat, leak, by = c('uuid', 'ad_target_ent'), all.x = T)
    dat[is.na(uuid_ent), uuid_ent := 0]
    rm(leak); #gc()
    
    # It's too rare to bother    
    # cat(date(), 'Expanded leaks: topics\n')
    # leak = unique(fread(paste0(tmpdir, '/user_topics.csv')))
    # setnames(leak, c('uuid', 'ad_target_top'))
    # leak[, uuid_top := 1]
    # dat = merge(dat, leak, by = c('uuid', 'ad_target_top'), all.x = T)
    # dat[is.na(uuid_top), uuid_top := 0]
    # rm(leak); gc()
  }
  
  cat(date(), 'Saving huge file\n')
  save(dat, file = paste0(tmpdir, '/ppdata0.RData'), compress = F)
  
  set.seed(123)
  dat.sample = dat[display_id %in% sample(unique(display_id), debug.ndisps)]
  save(dat.sample, file = paste0(tmpdir, '/ppdata0-sample.RData'))
  
  if (debug.mode && do.engineer1) {
    dat = dat.sample; rm(dat.sample)
  }  
} else if (do.engineer1) {
  if (debug.mode) {
    load(paste0(tmpdir, '/ppdata0-sample.RData'))
    dat = dat.sample; rm(dat.sample)
  } else {
    cat(date(), 'Loading huge file\n')
    load(paste0(tmpdir, '/ppdata0.RData'))
  }
}

if (do.engineer1) {
  cat(date(), 'Location features\n')
  dat[, country := substr(geo_location, 1, 2)]
  dat[, state   := substr(geo_location, 4, 5)]
  
  gc()
  
  cat(date(), 'Time features\n')
  dat[, time.utc := as.POSIXct((timestamp + 1465876799998) / 1000, origin = '1970-01-01', tz = 'UTC')]
  dat[, timestamp    := NULL]
  
  # FIXME assuming user in USA (can adjust per user, but vast majority of users are indeed in US+CA)
  #dat[, is.office.hour := as.integer(between(hour(time.utc - 6 * 3600), 8, 18))]
  
  dat[, disp_doc_pubtime  := as.POSIXct(disp_doc_pubtime , tz = 'EST', format = '%Y-%m-%d %H:%M:%S')]
  dat[, ad_target_pubtime := as.POSIXct(ad_target_pubtime, tz = 'EST', format = '%Y-%m-%d %H:%M:%S')]
  
  x = dat[, as.numeric(disp_doc_pubtime - ad_target_pubtime)]
  dat[, disp_ad_pub_atd := scale(log1p(abs(x)))]
  dat[, disp_ad_pub_std := sign(x > 0)]
  
  x = dat[, as.numeric(time.utc - ad_target_pubtime)]
  dat[, event_ad_pub_atd := scale(log1p(abs(x)))]
  dat[, event_ad_pub_std := sign(x)]
  
  cat(date(), 'Source-dest match features\n')
  
  #dat[, disp_ad_domain_match := as.integer(disp_doc_domain == ad_target_domain)] # matches only 370 times
  #dat[, disp_ad_subdomain_match := as.integer(disp_doc_subdomain == ad_target_subdomain)] # no matches!
  dat[, disp_ad_cat_match := as.numeric(disp_doc_cat == ad_target_cat)] # >26% matches
  dat[, disp_ad_ent_match := as.numeric(disp_doc_ent == ad_target_ent)] # about 1% matches, might be useful
  dat[, disp_ad_top_match := as.numeric(disp_doc_top == ad_target_top)] # TODO: check
  
  rm(x); gc()
  
  if (debug.mode) {
    save(dat, file = paste0(tmpdir, '/ppdata1-sample.RData'))
  } else {
    cat(date(), 'Saving huge file\n')
    save(dat, file = paste0(tmpdir, '/ppdata1.RData'), compress = F)
  }
} else if (do.engineer2) {
  if (debug.mode) {
    load(paste0(tmpdir, '/ppdata1-sample.RData'))
  } else {
    cat(date(), 'Loading huge file\n')
    load(paste0(tmpdir, '/ppdata1.RData'))
  }
}

if (do.engineer2) {
  cat(date(), 'Drop unused\n')
  dat[, disp_doc_pubtime    := NULL]
  dat[, ad_target_pubtime   := NULL]
  
  # !!!!!!!!!!!! FIXME I haven't tried these in debug mode. Try them, and if they help, add them to the big one.
  dat[, c('user.nr.views', 'user.nr.locations', 'user.nr.publishers', 'user.platform1', 'user.platform2', 'user.platform3', 'user.trafcsrc1', 'user.trafcsrc2', 'user.trafcsrc3') := NULL]
  
  gc()
  
  cat(date(), 'Count features\n')
  
  #dat[, display.size        := .N       , by = display_id    ]
  dat[, ad.evcnt            := log(.N), by = ad_id         ]
  dat[, ad.target.evcnt     := log(.N), by = ad_target     ]
  dat[, campaign_id.evcnt   := log(.N), by = campaign_id   ]
  dat[, advertiser_id.evcnt := log(.N), by = advertiser_id ]
  
  #dat[, uuid.evcnt                := log(uniqueN(display_id)), by = uuid               ]
  #dat[, country.evcnt             := log(uniqueN(display_id)), by = country            ]
  #dat[, state.evcnt               := log(uniqueN(display_id)), by = state              ]
  #dat[, geo_location.evcnt        := log(uniqueN(display_id)), by = geo_location       ]
  #dat[, disp_docid.evcnt          := log(uniqueN(display_id)), by = disp_docid         ]
  #dat[, disp_doc_domain.evcnt     := log(uniqueN(display_id)), by = disp_doc_domain    ]
  #dat[, disp_doc_subdomain.evcnt  := log(uniqueN(display_id)), by = disp_doc_subdomain ]
  #dat[, disp_doc_cat.evcnt        := log(uniqueN(display_id)), by = disp_doc_cat       ]
  #dat[, disp_doc_ent.evcnt        := log(uniqueN(display_id)), by = disp_doc_ent       ]
  #dat[, disp_doc_top.evcnt        := log(uniqueN(display_id)), by = disp_doc_top       ]
  #dat[, ad_target_domain.evcnt    := log(uniqueN(display_id)), by = ad_target_domain   ]
  #dat[, ad_target_subdomain.evcnt := log(uniqueN(display_id)), by = ad_target_subdomain]
  #dat[, ad_target_cat.evcnt       := log(uniqueN(display_id)), by = ad_target_cat      ]
  #dat[, ad_target_ent.evcnt       := log(uniqueN(display_id)), by = ad_target_ent      ]
  #dat[, ad_target_top.evcnt       := log(uniqueN(display_id)), by = ad_target_top      ]
  
  #dat[, uuid.ad.evcnt       := log(.N), by = .(uuid, ad_id           )]
  #dat[, uuid.targ.evcnt     := log(.N), by = .(uuid, ad_target       )]
  #dat[, uuid.camp.evcnt     := log(.N), by = .(uuid, campaign_id     )]
  #dat[, uuid.adv.evcnt      := log(.N), by = .(uuid, advertiser_id   )]
  #dat[, country.ad.evcnt    := log(.N), by = .(country, ad_id        )]
  #dat[, country.targ.evcnt  := log(.N), by = .(country, ad_target    )]
  #dat[, country.camp.evcnt  := log(.N), by = .(country, campaign_id  )]
  #dat[, country.adv.evcnt   := log(.N), by = .(country, advertiser_id)]
  #dat[, state.ad.evcnt      := log(.N), by = .(state, ad_id          )]
  #dat[, state.targ.evcnt    := log(.N), by = .(state, ad_target      )]
  #dat[, state.camp.evcnt    := log(.N), by = .(state, campaign_id    )]
  #dat[, state.adv.evcnt     := log(.N), by = .(state, advertiser_id  )]
  
  gc()
  
  # NOTE: this is dangerous. Let's hope it doesn't leak...
  cat(date(), 'Basal stacking meta-features\n')
  
  make.meta = function(x, y, g) {
    cat(date(), '\n')
    
    cidx = as.factor(x)
    nr.levels = nlevels(cidx) + anyNA(cidx)
    cidx = as.integer(cidx)
    cidx[is.na(cidx)] = nr.levels
    sdx = as(sparseMatrix(seq_len(length(x)), cidx, dims = c(length(x), nr.levels)), 'dgCMatrix')
    rm(nr.levels, cidx)
    
    train.mask = !is.na(y)
    train.ug = unique(g[train.mask])
    
    nr.folds = 3
    set.seed(123)
    cv.folds = createFolds(train.ug, k = nr.folds)
    
    # Well... this could be tuned per feature
    xgb.params = list(
      objective           = 'rank:map', 
      eval_metric         = 'map@12',
      maximize            = T,
      booster             = 'gblinear',
      nrounds             = 1000,
      eta                 = 1,
      #alpha               = 1,
      lambda              = 2,
      annoying = T
    )
    
    best.cv.iter = rep(NA, nr.folds)
    
    for (i in 1:nr.folds) {
      fold.valid.mask = g %in% train.ug[cv.folds[[i]]]
      fold.train.mask = train.mask & !fold.valid.mask
      fold.xtrain = xgb.DMatrix(sdx[fold.train.mask, ], label = y[fold.train.mask], group = rle(g[fold.train.mask])$lengths)
      fold.xvalid = xgb.DMatrix(sdx[fold.valid.mask, ], label = y[fold.valid.mask], group = rle(g[fold.valid.mask])$lengths)
      
      xgb = xgb.train(
        early_stopping_rounds = 20,
        nrounds   = xgb.params$nrounds,
        params    = xgb.params,
        maximize  = xgb.params$maximize,
        data      = fold.xtrain,
        watchlist = list(valid = fold.xvalid), # FIXME it'll be optimistic since I early-stop on the same validset
        verbose   = 0 #(i == 1)
      )
      
      x[fold.valid.mask] = predict(xgb, fold.xvalid, ntreelimit = 0)
      best.cv.iter[i] = xgb$best_iteration
      gc()
    }
    
    rm(fold.xtrain, fold.xvalid, fold.train.mask, fold.valid.mask); gc()
    
    xtrain = xgb.DMatrix(sdx[ train.mask, ], label = y[ train.mask], group = rle(g[ train.mask])$lengths)
    xtest  = xgb.DMatrix(sdx[!train.mask, ],                         group = rle(g[!train.mask])$lengths)
    
    xgb = xgb.train(
      nrounds   = round(mean(best.cv.iter)),
      params    = xgb.params,
      maximize  = xgb.params$maximize,
      data      = xtrain,
      verbose   = 0
    )
    
    x[!train.mask] = predict(xgb, xtest, ntreelimit = 0)
    
    return (x)
  }
  
  dat[, ai.meta := make.meta(ad_id        , clicked, display_id)]; gc()
  dat[, at.meta := make.meta(ad_target    , clicked, display_id)]; gc()
  dat[, ac.meta := make.meta(campaign_id  , clicked, display_id)]; gc()
  dat[, aa.meta := make.meta(advertiser_id, clicked, display_id)]; gc()
  
  #dat[, att.meta := make.meta(ad_target_top      , clicked, display_id)]; gc()
  #dat[, ate.meta := make.meta(ad_target_ent      , clicked, display_id)]; gc()
  #dat[, atc.meta := make.meta(ad_target_cat      , clicked, display_id)]; gc()
  #dat[, atd.meta := make.meta(ad_target_domain   , clicked, display_id)]; gc()
  #dat[, ats.meta := make.meta(ad_target_subdomain, clicked, display_id)]; gc()
  
  # NOTE: user features are by definition useless marginally.
  # I tried interactions with uuid, it's too sparse I guess
  dat[, upXai.meta := make.meta(as.integer(as.factor(paste0(platform       , ad_id        ))), clicked, display_id)]; gc()
  dat[, upXat.meta := make.meta(as.integer(as.factor(paste0(platform       , ad_target    ))), clicked, display_id)]; gc()
  dat[, upXac.meta := make.meta(as.integer(as.factor(paste0(platform       , campaign_id  ))), clicked, display_id)]; gc()
  dat[, upXaa.meta := make.meta(as.integer(as.factor(paste0(platform       , advertiser_id))), clicked, display_id)]; gc()
  dat[, ucXai.meta := make.meta(as.integer(as.factor(paste0(country        , ad_id        ))), clicked, display_id)]; gc()
  dat[, ucXat.meta := make.meta(as.integer(as.factor(paste0(country        , ad_target    ))), clicked, display_id)]; gc()
  dat[, ucXac.meta := make.meta(as.integer(as.factor(paste0(country        , campaign_id  ))), clicked, display_id)]; gc()
  dat[, ucXaa.meta := make.meta(as.integer(as.factor(paste0(country        , advertiser_id))), clicked, display_id)]; gc()
  #dat[, usXai.meta := make.meta(as.integer(as.factor(paste0(state          , ad_id        ))), clicked, display_id)]; gc() # all of these are slightly useful
  #dat[, usXat.meta := make.meta(as.integer(as.factor(paste0(state          , ad_target    ))), clicked, display_id)]; gc()
  #dat[, usXac.meta := make.meta(as.integer(as.factor(paste0(state          , campaign_id  ))), clicked, display_id)]; gc()
  #dat[, usXaa.meta := make.meta(as.integer(as.factor(paste0(state          , advertiser_id))), clicked, display_id)]; gc()
  #dat[, ugXai.meta := make.meta(as.integer(as.factor(paste0(geo_location   , ad_id        ))), clicked, display_id)]; gc()
  #dat[, ugXat.meta := make.meta(as.integer(as.factor(paste0(geo_location   , ad_target    ))), clicked, display_id)]; gc()
  #dat[, ugXac.meta := make.meta(as.integer(as.factor(paste0(geo_location   , campaign_id  ))), clicked, display_id)]; gc()
  #dat[, ugXaa.meta := make.meta(as.integer(as.factor(paste0(geo_location   , advertiser_id))), clicked, display_id)]; gc()
  dat[, diXai.meta := make.meta(as.integer(as.factor(paste0(disp_docid     , ad_id        ))), clicked, display_id)]; gc()
  dat[, diXat.meta := make.meta(as.integer(as.factor(paste0(disp_docid     , ad_target    ))), clicked, display_id)]; gc()
  dat[, diXac.meta := make.meta(as.integer(as.factor(paste0(disp_docid     , campaign_id  ))), clicked, display_id)]; gc()
  dat[, diXaa.meta := make.meta(as.integer(as.factor(paste0(disp_docid     , advertiser_id))), clicked, display_id)]; gc()
  dat.t = as.integer(dat$time.utc)
  dat.t = floor((dat.t - min(dat.t)) / diff(range(dat.t)) * 14/3) # FIXME tune the bin width... (it has to generalize to the last two test-only days, so better be more than 2/14)
  dat[, dtXai.meta := make.meta(as.integer(as.factor(paste0(dat.t          , ad_id        ))), clicked, display_id)]; gc()
  dat[, dtXat.meta := make.meta(as.integer(as.factor(paste0(dat.t          , ad_target    ))), clicked, display_id)]; gc()
  dat[, dtXac.meta := make.meta(as.integer(as.factor(paste0(dat.t          , campaign_id  ))), clicked, display_id)]; gc()
  dat[, dtXaa.meta := make.meta(as.integer(as.factor(paste0(dat.t          , advertiser_id))), clicked, display_id)]; gc()
  rm(dat.t)
  dat[, ddXai.meta := make.meta(as.integer(as.factor(paste0(disp_doc_domain, ad_id        ))), clicked, display_id)]; gc()
  dat[, ddXat.meta := make.meta(as.integer(as.factor(paste0(disp_doc_domain, ad_target    ))), clicked, display_id)]; gc()
  dat[, ddXac.meta := make.meta(as.integer(as.factor(paste0(disp_doc_domain, campaign_id  ))), clicked, display_id)]; gc()
  dat[, ddXaa.meta := make.meta(as.integer(as.factor(paste0(disp_doc_domain, advertiser_id))), clicked, display_id)]; gc()
  # TODO: more display/user and target interactions (by docs)
  # TODO: higher order interactions of the most important shit
  
  if (0) {
    cat(date(), 'Within-display transform for meta features (aka ad interactions)\n')
    
    setkey(dat, display_id)
    
    dat[, ai.meta    := ai.meta    / max(ai.meta   ), by = display_id]
    dat[, at.meta    := at.meta    / max(at.meta   ), by = display_id]
    dat[, ac.meta    := ac.meta    / max(ac.meta   ), by = display_id]
    dat[, aa.meta    := aa.meta    / max(aa.meta   ), by = display_id]
    
    dat[, upXai.meta := upXai.meta / max(upXai.meta), by = display_id]
    dat[, upXat.meta := upXat.meta / max(upXat.meta), by = display_id]
    dat[, upXac.meta := upXac.meta / max(upXac.meta), by = display_id]
    dat[, upXaa.meta := upXaa.meta / max(upXaa.meta), by = display_id]; gc()
    
    dat[, ucXai.meta := ucXai.meta / max(ucXai.meta), by = display_id]
    dat[, ucXat.meta := ucXat.meta / max(ucXat.meta), by = display_id]
    dat[, ucXac.meta := ucXac.meta / max(ucXac.meta), by = display_id]
    dat[, ucXaa.meta := ucXaa.meta / max(ucXaa.meta), by = display_id]
    
    dat[, diXai.meta := diXai.meta / max(diXai.meta), by = display_id]
    dat[, diXat.meta := diXat.meta / max(diXat.meta), by = display_id]
    dat[, diXac.meta := diXac.meta / max(diXac.meta), by = display_id]
    dat[, diXaa.meta := diXaa.meta / max(diXaa.meta), by = display_id]; gc()
    
    dat[, dtXai.meta := dtXai.meta / max(dtXai.meta), by = display_id]
    dat[, dtXat.meta := dtXat.meta / max(dtXat.meta), by = display_id]
    dat[, dtXac.meta := dtXac.meta / max(dtXac.meta), by = display_id]
    dat[, dtXaa.meta := dtXaa.meta / max(dtXaa.meta), by = display_id]
    
    dat[, ddXai.meta := ddXai.meta / max(ddXai.meta), by = display_id]
    dat[, ddXat.meta := ddXat.meta / max(ddXat.meta), by = display_id]
    dat[, ddXac.meta := ddXac.meta / max(ddXac.meta), by = display_id]
    dat[, ddXaa.meta := ddXaa.meta / max(ddXaa.meta), by = display_id]; gc()
  }
  
  cat(date(), 'Cast to factors\n')
  
  if (merge.rare.categories) {
    # Clumps all small classes together with NA
    # make.factor = function(x) {
    #   x = as.factor(x)
    #   tbl = table(x)
    #   levels.to.drop = names(tbl[tbl < rare.threshold])
    #   x[x %in% levels.to.drop] = NA # maybe best to leave original NAs as a separate category?
    #   return (factor(x))
    # }
    
    # Merges small classes with each other (this is similar to "feature hashing", but maybe smarter?)
    make.factor = function(x) {
      x = addNA(x, ifany = T)
      tbl = table(x)
      rare.level.idx = which(tbl < rare.threshold)
      nr.rare.levels = length(rare.level.idx)
      if (nr.rare.levels == 0) {
        return (x)
      } else if (nr.rare.levels == 1) {
        x[x == names(tbl)[rare.level.idx]] = NA
      } else {
        levels.to.merge.from = sample(rare.level.idx, ceiling(nr.rare.levels / 2))
        levels.to.merge.to = setdiff(rare.level.idx, levels.to.merge.from)
        if (length(levels.to.merge.to) != length(levels.to.merge.from)) levels.to.merge.to = c(levels.to.merge.to, levels.to.merge.to[1])
        levels.not.to.touch = setdiff(seq_along(tbl), levels.to.merge.from)
        levels.to.merge.from = c(levels.to.merge.from, levels.not.to.touch)
        levels.to.merge.to   = c(levels.to.merge.to  , levels.not.to.touch)[order(levels.to.merge.from)]
        x = levels.to.merge.to[as.integer(x)]
      }
      return (factor(x))
    }
  } else {
    make.factor = as.factor
  }
  
  if (keep.factors) {
    dat[, ad_id2        := make.factor(ad_id        )] # we need ad_id for a consistent sorting
    dat[, campaign_id   := make.factor(campaign_id  )]
    dat[, advertiser_id := make.factor(advertiser_id)]
    dat[, ad_target     := make.factor(ad_target    )]
    dat[, uuid          := make.factor(uuid         )]
    dat[, platform      := make.factor(platform     )]
    dat[, country       := make.factor(country      )]
    dat[, state         := make.factor(state        )]
    dat[, geo_location  := make.factor(geo_location )]
    dat[, disp_docid    := make.factor(disp_docid   )]
    gc()
    
    dat[, disp_doc_domain     := make.factor(disp_doc_domain    )]
    dat[, disp_doc_subdomain  := make.factor(disp_doc_subdomain )]
    dat[, disp_doc_cat        := make.factor(disp_doc_cat       )]
    dat[, disp_doc_ent        := make.factor(disp_doc_ent       )]
    dat[, disp_doc_top        := make.factor(disp_doc_top       )]
    dat[, ad_target_domain    := make.factor(ad_target_domain   )]
    dat[, ad_target_subdomain := make.factor(ad_target_subdomain)]
    dat[, ad_target_cat       := make.factor(ad_target_cat      )]
    dat[, ad_target_ent       := make.factor(ad_target_ent      )]
    dat[, ad_target_top       := make.factor(ad_target_top      )]
    gc()
  } else {
    dat[, (c(
      'campaign_id', 'advertiser_id', 'ad_target', 'uuid', 'platform', 'country', 'state', 'geo_location',
      'disp_docid', 'disp_doc_domain', 'disp_doc_subdomain', 'disp_doc_cat', 'disp_doc_ent', 'disp_doc_top', 
      'ad_target_domain', 'ad_target_subdomain', 'ad_target_cat', 'ad_target_ent', 'ad_target_top'
    )) := NULL]
  }
  
  # FIXME: some of these are probably worth a bit (0.001?) but I only have so much RAM
  dat[, (intersect(names(dat), c(
    'uuid_ent', 'event_ad_pub_std', 'disp_ad_ent_match', 'is.office.hour', 'disp_ad_top_match', 
    'uuid_cat', 'disp_ad_pub_std', 'uuid_top', 'disp_ad_cat_match', 'ad.display.idx', 'uuid_domain'
  ))) := NULL]
  
  cat(date(), 'Positive leak present in:', sum(dat[!is.na(clicked), leak]) / uniqueN(dat[!is.na(clicked), display_id]), 'of train, and', sum(dat[is.na(clicked), leak]) / uniqueN(dat[is.na(clicked), display_id]), 'of test.\n')
  # => this says about 0.044 of the task is "leaked". I think it's probably not golden unless we can
  # verify that the click is in fact the one from the display (using timestapms and traffic_source).
  # Assuming we get 0.66 score on non leaks, and 1 on leaks, this gives an overall score of 0.675.
  
  # if (add.expanded.leak) {
  #   cat(date(), 'Expanded leak 1 present in:', sum(dat[!is.na(clicked), uuid_campaign ]) / uniqueN(dat[!is.na(clicked), display_id]), 'of train, and', sum(dat[is.na(clicked), uuid_campaign ]) / uniqueN(dat[is.na(clicked), display_id]), 'of test.\n')
  #   cat(date(), 'Expanded leak 2 present in:', sum(dat[!is.na(clicked), uuid_domain   ]) / uniqueN(dat[!is.na(clicked), display_id]), 'of train, and', sum(dat[is.na(clicked), uuid_domain   ]) / uniqueN(dat[is.na(clicked), display_id]), 'of test.\n')
  #   cat(date(), 'Expanded leak 3 present in:', sum(dat[!is.na(clicked), uuid_subdomain]) / uniqueN(dat[!is.na(clicked), display_id]), 'of train, and', sum(dat[is.na(clicked), uuid_subdomain]) / uniqueN(dat[is.na(clicked), display_id]), 'of test.\n')
  #   cat(date(), 'Expanded leak 4 present in:', sum(dat[!is.na(clicked), uuid_cat      ]) / uniqueN(dat[!is.na(clicked), display_id]), 'of train, and', sum(dat[is.na(clicked), uuid_cat      ]) / uniqueN(dat[is.na(clicked), display_id]), 'of test.\n')
  #   cat(date(), 'Expanded leak 5 present in:', sum(dat[!is.na(clicked), uuid_ent      ]) / uniqueN(dat[!is.na(clicked), display_id]), 'of train, and', sum(dat[is.na(clicked), uuid_ent      ]) / uniqueN(dat[is.na(clicked), display_id]), 'of test.\n')
  #  #cat(date(), 'Expanded leak 6 present in:', sum(dat[!is.na(clicked), uuid_top      ]) / uniqueN(dat[!is.na(clicked), display_id]), 'of train, and', sum(dat[is.na(clicked), uuid_top      ]) / uniqueN(dat[is.na(clicked), display_id]), 'of test.\n')
  # }
  
  if (add.negative.leak) {
    tmp.train = train[, .(display_id, leak, tracked)]
    tmp.train[, sum.leak := sum(leak), by = display_id]
    tmp.train = tmp.train[sum.leak == 0, .(mean.track = mean(tracked)), by = display_id]
    tmp.test = test[, .(display_id, leak, tracked)]
    tmp.test[, sum.leak := sum(leak), by = display_id]
    tmp.test = tmp.test[sum.leak == 0, .(mean.track = mean(tracked)), by = display_id]
    cat(date(), 'Negative leak present in:', mean(tmp.train$mean.track > 0), 'of remaining train, and', mean(tmp.test$mean.track > 0), 'of remaining test.\n')
  }
  
  setorder(dat, display_id, ad_id)
  
  if (debug.mode) {
    save(dat, file = paste0(tmpdir, '/ppdata2-sample.RData'))
  } else {
    cat(date(), 'Saving huge file\n')
    save(dat, file = paste0(tmpdir, '/ppdata2.RData'), compress = F)
  }
}  

cat(date(), 'Done.\n')
